{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Federated Learning on MNIST using PyTorch + PySyft",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gargarchit/Federated-Learning-on-MNIST/blob/master/Federated_Learning_on_MNIST_using_PyTorch_%2B_PySyft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTzg2HdwPjUL",
        "colab_type": "text"
      },
      "source": [
        "Task: Federated Learning where the central server is not trusted with the raw gradients.\n",
        "\n",
        "**Need to train on the MNIST dataset using federated learning However the gradient should not come up to central server in raw form**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n59ChaCR0Uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgCacd5GRhGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tf-encrypted\n",
        "\n",
        "! URL=\"https://github.com/openmined/PySyft.git\" && FOLDER=\"PySyft\" && if [ ! -d $FOLDER ]; then git clone -b dev --single-branch $URL; else (cd $FOLDER && git pull $URL && cd ..); fi;\n",
        "\n",
        "!cd PySyft; python setup.py install  > /dev/null\n",
        "\n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('./PySyft'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "    \n",
        "!pip install --upgrade --force-reinstall lz4\n",
        "!pip install --upgrade --force-reinstall websocket\n",
        "!pip install --upgrade --force-reinstall websockets\n",
        "!pip install --upgrade --force-reinstall zstd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3FrYjQyHkF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import syft as sy\n",
        "hook = sy.TorchHook(torch)\n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")  #define remote worker bob\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETjSGBFnTDIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dfce5cf2-b7ca-4113-fbff-971eb9b1df8b"
      },
      "source": [
        "#We define the setting of the learning task\n",
        "\n",
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.log_interval = 30\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "torch.manual_seed(args.seed)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f96ff30b250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb0_fFNtEP1V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "566f5b57-9143-47b9-bf1b-89001f51b360"
      },
      "source": [
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK6lNkqYVzjE",
        "colab_type": "text"
      },
      "source": [
        "<h2>Data loading and sending to workers</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Z8C53wThHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FederatedDataLoader \n",
        "# we distribute the dataset across all the workers, it's now a FederatedDataset\n",
        "transform=transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.1307,), (0.3081,))]) \n",
        "mnist_trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform).federate((bob, alice))\n",
        "mnist_testset = datasets.MNIST(root='../data', train=False, download=True, transform=transform)\n",
        "\n",
        "federated_train_loader = sy.FederatedDataLoader(mnist_trainset, batch_size=args.batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=args.test_batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuO9ibk7WkV8",
        "colab_type": "text"
      },
      "source": [
        "<h2>Network Architecture</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA1KtpYCWQR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn, optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVnIl6qwWu77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Forward Convolutional Neural Network Architecture model\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuBrVxTV45J8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ecec0a8-2e01-4fed-b3d9-0d4b09be6013"
      },
      "source": [
        "model = Classifier()\n",
        "#model = model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader):\n",
        "        \n",
        "        model.send(data.location) #send the model to the right location\n",
        "        \n",
        "      #  data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.get()\n",
        "        \n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            loss = loss.get() #get the loss back\n",
        "            print('Epoch: {} [Training: {:.0f}%]\\tLoss: {:.6f}'.format(epoch, 100. * batch_idx / len(federated_train_loader), loss.item()))\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            \n",
        "         #   data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            \n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "if (args.save_model):\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 [Training: 0%]\tLoss: 2.305134\n",
            "Epoch: 1 [Training: 3%]\tLoss: 2.156802\n",
            "Epoch: 1 [Training: 6%]\tLoss: 1.896587\n",
            "Epoch: 1 [Training: 10%]\tLoss: 1.440329\n",
            "Epoch: 1 [Training: 13%]\tLoss: 0.867023\n",
            "Epoch: 1 [Training: 16%]\tLoss: 0.654317\n",
            "Epoch: 1 [Training: 19%]\tLoss: 0.593099\n",
            "Epoch: 1 [Training: 22%]\tLoss: 0.455322\n",
            "Epoch: 1 [Training: 26%]\tLoss: 0.371324\n",
            "Epoch: 1 [Training: 29%]\tLoss: 0.304617\n",
            "Epoch: 1 [Training: 32%]\tLoss: 0.314097\n",
            "Epoch: 1 [Training: 35%]\tLoss: 0.369211\n",
            "Epoch: 1 [Training: 38%]\tLoss: 0.238020\n",
            "Epoch: 1 [Training: 42%]\tLoss: 0.187752\n",
            "Epoch: 1 [Training: 45%]\tLoss: 0.523223\n",
            "Epoch: 1 [Training: 48%]\tLoss: 0.224323\n",
            "Epoch: 1 [Training: 51%]\tLoss: 0.143916\n",
            "Epoch: 1 [Training: 54%]\tLoss: 0.268499\n",
            "Epoch: 1 [Training: 58%]\tLoss: 0.187934\n",
            "Epoch: 1 [Training: 61%]\tLoss: 0.303621\n",
            "Epoch: 1 [Training: 64%]\tLoss: 0.239960\n",
            "Epoch: 1 [Training: 67%]\tLoss: 0.256438\n",
            "Epoch: 1 [Training: 70%]\tLoss: 0.191681\n",
            "Epoch: 1 [Training: 74%]\tLoss: 0.174667\n",
            "Epoch: 1 [Training: 77%]\tLoss: 0.220412\n",
            "Epoch: 1 [Training: 80%]\tLoss: 0.323814\n",
            "Epoch: 1 [Training: 83%]\tLoss: 0.273938\n",
            "Epoch: 1 [Training: 86%]\tLoss: 0.129327\n",
            "Epoch: 1 [Training: 90%]\tLoss: 0.182833\n",
            "Epoch: 1 [Training: 93%]\tLoss: 0.223088\n",
            "Epoch: 1 [Training: 96%]\tLoss: 0.081163\n",
            "Epoch: 1 [Training: 99%]\tLoss: 0.143191\n",
            "\n",
            "Test set: Average loss: 0.1575, Accuracy: 9511/10000 (95%)\n",
            "\n",
            "Epoch: 2 [Training: 0%]\tLoss: 0.103221\n",
            "Epoch: 2 [Training: 3%]\tLoss: 0.105845\n",
            "Epoch: 2 [Training: 6%]\tLoss: 0.147023\n",
            "Epoch: 2 [Training: 10%]\tLoss: 0.148321\n",
            "Epoch: 2 [Training: 13%]\tLoss: 0.108846\n",
            "Epoch: 2 [Training: 16%]\tLoss: 0.111557\n",
            "Epoch: 2 [Training: 19%]\tLoss: 0.118514\n",
            "Epoch: 2 [Training: 22%]\tLoss: 0.063139\n",
            "Epoch: 2 [Training: 26%]\tLoss: 0.089060\n",
            "Epoch: 2 [Training: 29%]\tLoss: 0.156941\n",
            "Epoch: 2 [Training: 32%]\tLoss: 0.159745\n",
            "Epoch: 2 [Training: 35%]\tLoss: 0.157456\n",
            "Epoch: 2 [Training: 38%]\tLoss: 0.229732\n",
            "Epoch: 2 [Training: 42%]\tLoss: 0.197346\n",
            "Epoch: 2 [Training: 45%]\tLoss: 0.206655\n",
            "Epoch: 2 [Training: 48%]\tLoss: 0.079507\n",
            "Epoch: 2 [Training: 51%]\tLoss: 0.063148\n",
            "Epoch: 2 [Training: 54%]\tLoss: 0.158469\n",
            "Epoch: 2 [Training: 58%]\tLoss: 0.156446\n",
            "Epoch: 2 [Training: 61%]\tLoss: 0.074369\n",
            "Epoch: 2 [Training: 64%]\tLoss: 0.161415\n",
            "Epoch: 2 [Training: 67%]\tLoss: 0.074256\n",
            "Epoch: 2 [Training: 70%]\tLoss: 0.153609\n",
            "Epoch: 2 [Training: 74%]\tLoss: 0.048192\n",
            "Epoch: 2 [Training: 77%]\tLoss: 0.085225\n",
            "Epoch: 2 [Training: 80%]\tLoss: 0.101033\n",
            "Epoch: 2 [Training: 83%]\tLoss: 0.154688\n",
            "Epoch: 2 [Training: 86%]\tLoss: 0.032838\n",
            "Epoch: 2 [Training: 90%]\tLoss: 0.073842\n",
            "Epoch: 2 [Training: 93%]\tLoss: 0.113492\n",
            "Epoch: 2 [Training: 96%]\tLoss: 0.111734\n",
            "Epoch: 2 [Training: 99%]\tLoss: 0.069066\n",
            "\n",
            "Test set: Average loss: 0.0899, Accuracy: 9735/10000 (97%)\n",
            "\n",
            "Epoch: 3 [Training: 0%]\tLoss: 0.081291\n",
            "Epoch: 3 [Training: 3%]\tLoss: 0.080039\n",
            "Epoch: 3 [Training: 6%]\tLoss: 0.184774\n",
            "Epoch: 3 [Training: 10%]\tLoss: 0.072767\n",
            "Epoch: 3 [Training: 13%]\tLoss: 0.078837\n",
            "Epoch: 3 [Training: 16%]\tLoss: 0.206170\n",
            "Epoch: 3 [Training: 19%]\tLoss: 0.249777\n",
            "Epoch: 3 [Training: 22%]\tLoss: 0.121717\n",
            "Epoch: 3 [Training: 26%]\tLoss: 0.178415\n",
            "Epoch: 3 [Training: 29%]\tLoss: 0.150552\n",
            "Epoch: 3 [Training: 32%]\tLoss: 0.028152\n",
            "Epoch: 3 [Training: 35%]\tLoss: 0.046844\n",
            "Epoch: 3 [Training: 38%]\tLoss: 0.055311\n",
            "Epoch: 3 [Training: 42%]\tLoss: 0.034426\n",
            "Epoch: 3 [Training: 45%]\tLoss: 0.291176\n",
            "Epoch: 3 [Training: 48%]\tLoss: 0.055141\n",
            "Epoch: 3 [Training: 51%]\tLoss: 0.064245\n",
            "Epoch: 3 [Training: 54%]\tLoss: 0.025072\n",
            "Epoch: 3 [Training: 58%]\tLoss: 0.071311\n",
            "Epoch: 3 [Training: 61%]\tLoss: 0.039723\n",
            "Epoch: 3 [Training: 64%]\tLoss: 0.034051\n",
            "Epoch: 3 [Training: 67%]\tLoss: 0.082193\n",
            "Epoch: 3 [Training: 70%]\tLoss: 0.169696\n",
            "Epoch: 3 [Training: 74%]\tLoss: 0.054504\n",
            "Epoch: 3 [Training: 77%]\tLoss: 0.074233\n",
            "Epoch: 3 [Training: 80%]\tLoss: 0.024500\n",
            "Epoch: 3 [Training: 83%]\tLoss: 0.099400\n",
            "Epoch: 3 [Training: 86%]\tLoss: 0.026502\n",
            "Epoch: 3 [Training: 90%]\tLoss: 0.055414\n",
            "Epoch: 3 [Training: 93%]\tLoss: 0.067579\n",
            "Epoch: 3 [Training: 96%]\tLoss: 0.057820\n",
            "Epoch: 3 [Training: 99%]\tLoss: 0.463102\n",
            "\n",
            "Test set: Average loss: 0.0738, Accuracy: 9758/10000 (98%)\n",
            "\n",
            "Epoch: 4 [Training: 0%]\tLoss: 0.146829\n",
            "Epoch: 4 [Training: 3%]\tLoss: 0.065596\n",
            "Epoch: 4 [Training: 6%]\tLoss: 0.045302\n",
            "Epoch: 4 [Training: 10%]\tLoss: 0.101460\n",
            "Epoch: 4 [Training: 13%]\tLoss: 0.038305\n",
            "Epoch: 4 [Training: 16%]\tLoss: 0.068318\n",
            "Epoch: 4 [Training: 19%]\tLoss: 0.116785\n",
            "Epoch: 4 [Training: 22%]\tLoss: 0.105321\n",
            "Epoch: 4 [Training: 26%]\tLoss: 0.047861\n",
            "Epoch: 4 [Training: 29%]\tLoss: 0.048203\n",
            "Epoch: 4 [Training: 32%]\tLoss: 0.075156\n",
            "Epoch: 4 [Training: 35%]\tLoss: 0.083796\n",
            "Epoch: 4 [Training: 38%]\tLoss: 0.024896\n",
            "Epoch: 4 [Training: 42%]\tLoss: 0.143771\n",
            "Epoch: 4 [Training: 45%]\tLoss: 0.175964\n",
            "Epoch: 4 [Training: 48%]\tLoss: 0.093517\n",
            "Epoch: 4 [Training: 51%]\tLoss: 0.018805\n",
            "Epoch: 4 [Training: 54%]\tLoss: 0.150391\n",
            "Epoch: 4 [Training: 58%]\tLoss: 0.029585\n",
            "Epoch: 4 [Training: 61%]\tLoss: 0.009927\n",
            "Epoch: 4 [Training: 64%]\tLoss: 0.044069\n",
            "Epoch: 4 [Training: 67%]\tLoss: 0.049970\n",
            "Epoch: 4 [Training: 70%]\tLoss: 0.135398\n",
            "Epoch: 4 [Training: 74%]\tLoss: 0.057950\n",
            "Epoch: 4 [Training: 77%]\tLoss: 0.020599\n",
            "Epoch: 4 [Training: 80%]\tLoss: 0.040647\n",
            "Epoch: 4 [Training: 83%]\tLoss: 0.166192\n",
            "Epoch: 4 [Training: 86%]\tLoss: 0.067753\n",
            "Epoch: 4 [Training: 90%]\tLoss: 0.064018\n",
            "Epoch: 4 [Training: 93%]\tLoss: 0.073588\n",
            "Epoch: 4 [Training: 96%]\tLoss: 0.061888\n",
            "Epoch: 4 [Training: 99%]\tLoss: 0.017513\n",
            "\n",
            "Test set: Average loss: 0.0548, Accuracy: 9813/10000 (98%)\n",
            "\n",
            "Epoch: 5 [Training: 0%]\tLoss: 0.082590\n",
            "Epoch: 5 [Training: 3%]\tLoss: 0.040117\n",
            "Epoch: 5 [Training: 6%]\tLoss: 0.028699\n",
            "Epoch: 5 [Training: 10%]\tLoss: 0.035378\n",
            "Epoch: 5 [Training: 13%]\tLoss: 0.063442\n",
            "Epoch: 5 [Training: 16%]\tLoss: 0.022147\n",
            "Epoch: 5 [Training: 19%]\tLoss: 0.165380\n",
            "Epoch: 5 [Training: 22%]\tLoss: 0.046082\n",
            "Epoch: 5 [Training: 26%]\tLoss: 0.101586\n",
            "Epoch: 5 [Training: 29%]\tLoss: 0.054320\n",
            "Epoch: 5 [Training: 32%]\tLoss: 0.034714\n",
            "Epoch: 5 [Training: 35%]\tLoss: 0.097715\n",
            "Epoch: 5 [Training: 38%]\tLoss: 0.032395\n",
            "Epoch: 5 [Training: 42%]\tLoss: 0.026322\n",
            "Epoch: 5 [Training: 45%]\tLoss: 0.022107\n",
            "Epoch: 5 [Training: 48%]\tLoss: 0.017720\n",
            "Epoch: 5 [Training: 51%]\tLoss: 0.039595\n",
            "Epoch: 5 [Training: 54%]\tLoss: 0.069773\n",
            "Epoch: 5 [Training: 58%]\tLoss: 0.113465\n",
            "Epoch: 5 [Training: 61%]\tLoss: 0.037761\n",
            "Epoch: 5 [Training: 64%]\tLoss: 0.029371\n",
            "Epoch: 5 [Training: 67%]\tLoss: 0.011653\n",
            "Epoch: 5 [Training: 70%]\tLoss: 0.026142\n",
            "Epoch: 5 [Training: 74%]\tLoss: 0.017510\n",
            "Epoch: 5 [Training: 77%]\tLoss: 0.023011\n",
            "Epoch: 5 [Training: 80%]\tLoss: 0.015553\n",
            "Epoch: 5 [Training: 83%]\tLoss: 0.062981\n",
            "Epoch: 5 [Training: 86%]\tLoss: 0.039655\n",
            "Epoch: 5 [Training: 90%]\tLoss: 0.016948\n",
            "Epoch: 5 [Training: 93%]\tLoss: 0.097401\n",
            "Epoch: 5 [Training: 96%]\tLoss: 0.027165\n",
            "Epoch: 5 [Training: 99%]\tLoss: 0.061822\n",
            "\n",
            "Test set: Average loss: 0.0460, Accuracy: 9849/10000 (98%)\n",
            "\n",
            "Epoch: 6 [Training: 0%]\tLoss: 0.052845\n",
            "Epoch: 6 [Training: 3%]\tLoss: 0.101472\n",
            "Epoch: 6 [Training: 6%]\tLoss: 0.011245\n",
            "Epoch: 6 [Training: 10%]\tLoss: 0.090635\n",
            "Epoch: 6 [Training: 13%]\tLoss: 0.111328\n",
            "Epoch: 6 [Training: 16%]\tLoss: 0.014489\n",
            "Epoch: 6 [Training: 19%]\tLoss: 0.086852\n",
            "Epoch: 6 [Training: 22%]\tLoss: 0.036762\n",
            "Epoch: 6 [Training: 26%]\tLoss: 0.097659\n",
            "Epoch: 6 [Training: 29%]\tLoss: 0.053820\n",
            "Epoch: 6 [Training: 32%]\tLoss: 0.042853\n",
            "Epoch: 6 [Training: 35%]\tLoss: 0.073565\n",
            "Epoch: 6 [Training: 38%]\tLoss: 0.048715\n",
            "Epoch: 6 [Training: 42%]\tLoss: 0.007704\n",
            "Epoch: 6 [Training: 45%]\tLoss: 0.019984\n",
            "Epoch: 6 [Training: 48%]\tLoss: 0.036293\n",
            "Epoch: 6 [Training: 51%]\tLoss: 0.087016\n",
            "Epoch: 6 [Training: 54%]\tLoss: 0.010435\n",
            "Epoch: 6 [Training: 58%]\tLoss: 0.026880\n",
            "Epoch: 6 [Training: 61%]\tLoss: 0.048298\n",
            "Epoch: 6 [Training: 64%]\tLoss: 0.006616\n",
            "Epoch: 6 [Training: 67%]\tLoss: 0.076249\n",
            "Epoch: 6 [Training: 70%]\tLoss: 0.041769\n",
            "Epoch: 6 [Training: 74%]\tLoss: 0.007849\n",
            "Epoch: 6 [Training: 77%]\tLoss: 0.064405\n",
            "Epoch: 6 [Training: 80%]\tLoss: 0.051831\n",
            "Epoch: 6 [Training: 83%]\tLoss: 0.024883\n",
            "Epoch: 6 [Training: 86%]\tLoss: 0.031440\n",
            "Epoch: 6 [Training: 90%]\tLoss: 0.053846\n",
            "Epoch: 6 [Training: 93%]\tLoss: 0.024572\n",
            "Epoch: 6 [Training: 96%]\tLoss: 0.099601\n",
            "Epoch: 6 [Training: 99%]\tLoss: 0.012264\n",
            "\n",
            "Test set: Average loss: 0.0441, Accuracy: 9859/10000 (99%)\n",
            "\n",
            "Epoch: 7 [Training: 0%]\tLoss: 0.047999\n",
            "Epoch: 7 [Training: 3%]\tLoss: 0.022044\n",
            "Epoch: 7 [Training: 6%]\tLoss: 0.012739\n",
            "Epoch: 7 [Training: 10%]\tLoss: 0.040412\n",
            "Epoch: 7 [Training: 13%]\tLoss: 0.109332\n",
            "Epoch: 7 [Training: 16%]\tLoss: 0.027032\n",
            "Epoch: 7 [Training: 19%]\tLoss: 0.075890\n",
            "Epoch: 7 [Training: 22%]\tLoss: 0.019882\n",
            "Epoch: 7 [Training: 26%]\tLoss: 0.027513\n",
            "Epoch: 7 [Training: 29%]\tLoss: 0.163230\n",
            "Epoch: 7 [Training: 32%]\tLoss: 0.091138\n",
            "Epoch: 7 [Training: 35%]\tLoss: 0.063134\n",
            "Epoch: 7 [Training: 38%]\tLoss: 0.014684\n",
            "Epoch: 7 [Training: 42%]\tLoss: 0.083485\n",
            "Epoch: 7 [Training: 45%]\tLoss: 0.021518\n",
            "Epoch: 7 [Training: 48%]\tLoss: 0.012901\n",
            "Epoch: 7 [Training: 51%]\tLoss: 0.009388\n",
            "Epoch: 7 [Training: 54%]\tLoss: 0.058238\n",
            "Epoch: 7 [Training: 58%]\tLoss: 0.024058\n",
            "Epoch: 7 [Training: 61%]\tLoss: 0.029954\n",
            "Epoch: 7 [Training: 64%]\tLoss: 0.043035\n",
            "Epoch: 7 [Training: 67%]\tLoss: 0.035829\n",
            "Epoch: 7 [Training: 70%]\tLoss: 0.012116\n",
            "Epoch: 7 [Training: 74%]\tLoss: 0.009162\n",
            "Epoch: 7 [Training: 77%]\tLoss: 0.059630\n",
            "Epoch: 7 [Training: 80%]\tLoss: 0.044709\n",
            "Epoch: 7 [Training: 83%]\tLoss: 0.070452\n",
            "Epoch: 7 [Training: 86%]\tLoss: 0.037277\n",
            "Epoch: 7 [Training: 90%]\tLoss: 0.033389\n",
            "Epoch: 7 [Training: 93%]\tLoss: 0.021567\n",
            "Epoch: 7 [Training: 96%]\tLoss: 0.176802\n",
            "Epoch: 7 [Training: 99%]\tLoss: 0.026903\n",
            "\n",
            "Test set: Average loss: 0.0446, Accuracy: 9862/10000 (99%)\n",
            "\n",
            "Epoch: 8 [Training: 0%]\tLoss: 0.056659\n",
            "Epoch: 8 [Training: 3%]\tLoss: 0.038179\n",
            "Epoch: 8 [Training: 6%]\tLoss: 0.075729\n",
            "Epoch: 8 [Training: 10%]\tLoss: 0.080880\n",
            "Epoch: 8 [Training: 13%]\tLoss: 0.017966\n",
            "Epoch: 8 [Training: 16%]\tLoss: 0.011010\n",
            "Epoch: 8 [Training: 19%]\tLoss: 0.032439\n",
            "Epoch: 8 [Training: 22%]\tLoss: 0.135810\n",
            "Epoch: 8 [Training: 26%]\tLoss: 0.008220\n",
            "Epoch: 8 [Training: 29%]\tLoss: 0.031869\n",
            "Epoch: 8 [Training: 32%]\tLoss: 0.024362\n",
            "Epoch: 8 [Training: 35%]\tLoss: 0.003008\n",
            "Epoch: 8 [Training: 38%]\tLoss: 0.094172\n",
            "Epoch: 8 [Training: 42%]\tLoss: 0.046612\n",
            "Epoch: 8 [Training: 45%]\tLoss: 0.002846\n",
            "Epoch: 8 [Training: 48%]\tLoss: 0.002485\n",
            "Epoch: 8 [Training: 51%]\tLoss: 0.041518\n",
            "Epoch: 8 [Training: 54%]\tLoss: 0.058854\n",
            "Epoch: 8 [Training: 58%]\tLoss: 0.016703\n",
            "Epoch: 8 [Training: 61%]\tLoss: 0.031350\n",
            "Epoch: 8 [Training: 64%]\tLoss: 0.068506\n",
            "Epoch: 8 [Training: 67%]\tLoss: 0.045703\n",
            "Epoch: 8 [Training: 70%]\tLoss: 0.046704\n",
            "Epoch: 8 [Training: 74%]\tLoss: 0.019379\n",
            "Epoch: 8 [Training: 77%]\tLoss: 0.055802\n",
            "Epoch: 8 [Training: 80%]\tLoss: 0.028705\n",
            "Epoch: 8 [Training: 83%]\tLoss: 0.014061\n",
            "Epoch: 8 [Training: 86%]\tLoss: 0.041628\n",
            "Epoch: 8 [Training: 90%]\tLoss: 0.037571\n",
            "Epoch: 8 [Training: 93%]\tLoss: 0.010487\n",
            "Epoch: 8 [Training: 96%]\tLoss: 0.081713\n",
            "Epoch: 8 [Training: 99%]\tLoss: 0.138579\n",
            "\n",
            "Test set: Average loss: 0.0362, Accuracy: 9886/10000 (99%)\n",
            "\n",
            "Epoch: 9 [Training: 0%]\tLoss: 0.005221\n",
            "Epoch: 9 [Training: 3%]\tLoss: 0.011011\n",
            "Epoch: 9 [Training: 6%]\tLoss: 0.019707\n",
            "Epoch: 9 [Training: 10%]\tLoss: 0.009392\n",
            "Epoch: 9 [Training: 13%]\tLoss: 0.062374\n",
            "Epoch: 9 [Training: 16%]\tLoss: 0.017745\n",
            "Epoch: 9 [Training: 19%]\tLoss: 0.070276\n",
            "Epoch: 9 [Training: 22%]\tLoss: 0.006879\n",
            "Epoch: 9 [Training: 26%]\tLoss: 0.021353\n",
            "Epoch: 9 [Training: 29%]\tLoss: 0.040855\n",
            "Epoch: 9 [Training: 32%]\tLoss: 0.022991\n",
            "Epoch: 9 [Training: 35%]\tLoss: 0.007182\n",
            "Epoch: 9 [Training: 38%]\tLoss: 0.071696\n",
            "Epoch: 9 [Training: 42%]\tLoss: 0.030944\n",
            "Epoch: 9 [Training: 45%]\tLoss: 0.034639\n",
            "Epoch: 9 [Training: 48%]\tLoss: 0.127856\n",
            "Epoch: 9 [Training: 51%]\tLoss: 0.044178\n",
            "Epoch: 9 [Training: 54%]\tLoss: 0.049925\n",
            "Epoch: 9 [Training: 58%]\tLoss: 0.028268\n",
            "Epoch: 9 [Training: 61%]\tLoss: 0.034851\n",
            "Epoch: 9 [Training: 64%]\tLoss: 0.010666\n",
            "Epoch: 9 [Training: 67%]\tLoss: 0.021169\n",
            "Epoch: 9 [Training: 70%]\tLoss: 0.031291\n",
            "Epoch: 9 [Training: 74%]\tLoss: 0.004419\n",
            "Epoch: 9 [Training: 77%]\tLoss: 0.049544\n",
            "Epoch: 9 [Training: 80%]\tLoss: 0.013750\n",
            "Epoch: 9 [Training: 83%]\tLoss: 0.006207\n",
            "Epoch: 9 [Training: 86%]\tLoss: 0.072524\n",
            "Epoch: 9 [Training: 90%]\tLoss: 0.019514\n",
            "Epoch: 9 [Training: 93%]\tLoss: 0.006574\n",
            "Epoch: 9 [Training: 96%]\tLoss: 0.010870\n",
            "Epoch: 9 [Training: 99%]\tLoss: 0.006324\n",
            "\n",
            "Test set: Average loss: 0.0344, Accuracy: 9891/10000 (99%)\n",
            "\n",
            "Epoch: 10 [Training: 0%]\tLoss: 0.004203\n",
            "Epoch: 10 [Training: 3%]\tLoss: 0.030270\n",
            "Epoch: 10 [Training: 6%]\tLoss: 0.029526\n",
            "Epoch: 10 [Training: 10%]\tLoss: 0.002077\n",
            "Epoch: 10 [Training: 13%]\tLoss: 0.088129\n",
            "Epoch: 10 [Training: 16%]\tLoss: 0.031981\n",
            "Epoch: 10 [Training: 19%]\tLoss: 0.037131\n",
            "Epoch: 10 [Training: 22%]\tLoss: 0.009185\n",
            "Epoch: 10 [Training: 26%]\tLoss: 0.002562\n",
            "Epoch: 10 [Training: 29%]\tLoss: 0.035339\n",
            "Epoch: 10 [Training: 32%]\tLoss: 0.146149\n",
            "Epoch: 10 [Training: 35%]\tLoss: 0.034385\n",
            "Epoch: 10 [Training: 38%]\tLoss: 0.189797\n",
            "Epoch: 10 [Training: 42%]\tLoss: 0.009782\n",
            "Epoch: 10 [Training: 45%]\tLoss: 0.012849\n",
            "Epoch: 10 [Training: 48%]\tLoss: 0.008088\n",
            "Epoch: 10 [Training: 51%]\tLoss: 0.013652\n",
            "Epoch: 10 [Training: 54%]\tLoss: 0.027806\n",
            "Epoch: 10 [Training: 58%]\tLoss: 0.033577\n",
            "Epoch: 10 [Training: 61%]\tLoss: 0.017015\n",
            "Epoch: 10 [Training: 64%]\tLoss: 0.028350\n",
            "Epoch: 10 [Training: 67%]\tLoss: 0.078473\n",
            "Epoch: 10 [Training: 70%]\tLoss: 0.014510\n",
            "Epoch: 10 [Training: 74%]\tLoss: 0.004908\n",
            "Epoch: 10 [Training: 77%]\tLoss: 0.004810\n",
            "Epoch: 10 [Training: 80%]\tLoss: 0.076611\n",
            "Epoch: 10 [Training: 83%]\tLoss: 0.043203\n",
            "Epoch: 10 [Training: 86%]\tLoss: 0.004271\n",
            "Epoch: 10 [Training: 90%]\tLoss: 0.100702\n",
            "Epoch: 10 [Training: 93%]\tLoss: 0.003043\n",
            "Epoch: 10 [Training: 96%]\tLoss: 0.013565\n",
            "Epoch: 10 [Training: 99%]\tLoss: 0.007521\n",
            "\n",
            "Test set: Average loss: 0.0376, Accuracy: 9872/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14Q8wcO55gF6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5b022c6-8947-4fb1-cb21-75e26d15aa5d"
      },
      "source": [
        "print(\"Accuracy Obtained {:.4f}%\".format( 100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Obtained 98.7200%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yUrVclGSMXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}