{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Federated Learning on MNIST using PyTorch + PySyft",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gargarchit/Federated-Learning-on-MNIST/blob/master/Federated_Learning_on_MNIST_using_PyTorch_%2B_PySyft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTzg2HdwPjUL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Need to train on the MNIST dataset using federated learning However the gradient should not come up to central server in raw form**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n59ChaCR0Uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiHUPcBrtp2t",
        "colab_type": "text"
      },
      "source": [
        "<h2>Installing and Importing PySyft"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgCacd5GRhGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tf-encrypted\n",
        "\n",
        "! URL=\"https://github.com/openmined/PySyft.git\" && FOLDER=\"PySyft\" && if [ ! -d $FOLDER ]; then git clone -b dev --single-branch $URL; else (cd $FOLDER && git pull $URL && cd ..); fi;\n",
        "\n",
        "!cd PySyft; python setup.py install  > /dev/null\n",
        "\n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('./PySyft'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "    \n",
        "!pip install --upgrade --force-reinstall lz4\n",
        "!pip install --upgrade --force-reinstall websocket\n",
        "!pip install --upgrade --force-reinstall websockets\n",
        "!pip install --upgrade --force-reinstall zstd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3FrYjQyHkF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import syft as sy\n",
        "hook = sy.TorchHook(torch)\n",
        "\n",
        "# Create a couple of workers\n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")  \n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETjSGBFnTDIv",
        "colab_type": "code",
        "outputId": "dfce5cf2-b7ca-4113-fbff-971eb9b1df8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#We define the setting of the learning task\n",
        "\n",
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.log_interval = 30\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "torch.manual_seed(args.seed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f96ff30b250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb0_fFNtEP1V",
        "colab_type": "code",
        "outputId": "566f5b57-9143-47b9-bf1b-89001f51b360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK6lNkqYVzjE",
        "colab_type": "text"
      },
      "source": [
        "<h2>Data loading and sending to workers</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Z8C53wThHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FederatedDataLoader \n",
        "# we distribute the dataset across all the workers, it's now a FederatedDataset\n",
        "transform=transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.1307,), (0.3081,))]) \n",
        "mnist_trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform).federate((bob, alice))\n",
        "mnist_testset = datasets.MNIST(root='../data', train=False, download=True, transform=transform)\n",
        "\n",
        "federated_train_loader = sy.FederatedDataLoader(mnist_trainset, batch_size=args.batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=args.test_batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuO9ibk7WkV8",
        "colab_type": "text"
      },
      "source": [
        "<h2>Network Architecture</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA1KtpYCWQR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVnIl6qwWu77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Forward Convolutional Neural Network Architecture model\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuBrVxTV45J8",
        "colab_type": "code",
        "outputId": "5dff9004-b097-444e-c2f7-ed5b89218ee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Classifier()\n",
        "model = model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader): # iterate through each worker's dataset\n",
        "        \n",
        "        model.send(data.location) #send the model to the right location\n",
        "        \n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad() # 1) erase previous gradients (if they exist)\n",
        "        output = model(data)  # 2) make a prediction\n",
        "        loss = F.nll_loss(output, target)  # 3) calculate how much we missed\n",
        "        loss.backward()  # 4) figure out which weights caused us to miss\n",
        "        optimizer.step()  # 5) change those weights\n",
        "        model.get()  # get the model back (with gradients)\n",
        "        \n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            loss = loss.get() #get the loss back\n",
        "            print('Epoch: {} [Training: {:.0f}%]\\tLoss: {:.6f}'.format(epoch, 100. * batch_idx / len(federated_train_loader), loss.item()))\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            \n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            \n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "if (args.save_model):\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 [Training: 0%]\tLoss: 2.302580\n",
            "Epoch: 1 [Training: 3%]\tLoss: 2.173574\n",
            "Epoch: 1 [Training: 6%]\tLoss: 1.956030\n",
            "Epoch: 1 [Training: 10%]\tLoss: 1.320343\n",
            "Epoch: 1 [Training: 13%]\tLoss: 0.835926\n",
            "Epoch: 1 [Training: 16%]\tLoss: 0.661665\n",
            "Epoch: 1 [Training: 19%]\tLoss: 0.719910\n",
            "Epoch: 1 [Training: 22%]\tLoss: 0.372965\n",
            "Epoch: 1 [Training: 26%]\tLoss: 0.568405\n",
            "Epoch: 1 [Training: 29%]\tLoss: 0.376864\n",
            "Epoch: 1 [Training: 32%]\tLoss: 0.306380\n",
            "Epoch: 1 [Training: 35%]\tLoss: 0.180607\n",
            "Epoch: 1 [Training: 38%]\tLoss: 0.434034\n",
            "Epoch: 1 [Training: 42%]\tLoss: 0.366301\n",
            "Epoch: 1 [Training: 45%]\tLoss: 0.284107\n",
            "Epoch: 1 [Training: 48%]\tLoss: 0.230000\n",
            "Epoch: 1 [Training: 51%]\tLoss: 0.243508\n",
            "Epoch: 1 [Training: 54%]\tLoss: 0.344105\n",
            "Epoch: 1 [Training: 58%]\tLoss: 0.269959\n",
            "Epoch: 1 [Training: 61%]\tLoss: 0.132271\n",
            "Epoch: 1 [Training: 64%]\tLoss: 0.138609\n",
            "Epoch: 1 [Training: 67%]\tLoss: 0.174517\n",
            "Epoch: 1 [Training: 70%]\tLoss: 0.082769\n",
            "Epoch: 1 [Training: 74%]\tLoss: 0.164831\n",
            "Epoch: 1 [Training: 77%]\tLoss: 0.207887\n",
            "Epoch: 1 [Training: 80%]\tLoss: 0.266643\n",
            "Epoch: 1 [Training: 83%]\tLoss: 0.168704\n",
            "Epoch: 1 [Training: 86%]\tLoss: 0.167106\n",
            "Epoch: 1 [Training: 90%]\tLoss: 0.191734\n",
            "Epoch: 1 [Training: 93%]\tLoss: 0.223882\n",
            "Epoch: 1 [Training: 96%]\tLoss: 0.245662\n",
            "Epoch: 1 [Training: 99%]\tLoss: 0.437968\n",
            "\n",
            "Test set: Average loss: 0.1513, Accuracy: 9559/10000 (96%)\n",
            "\n",
            "Epoch: 2 [Training: 0%]\tLoss: 0.036296\n",
            "Epoch: 2 [Training: 3%]\tLoss: 0.111524\n",
            "Epoch: 2 [Training: 6%]\tLoss: 0.083447\n",
            "Epoch: 2 [Training: 10%]\tLoss: 0.085515\n",
            "Epoch: 2 [Training: 13%]\tLoss: 0.156729\n",
            "Epoch: 2 [Training: 16%]\tLoss: 0.154421\n",
            "Epoch: 2 [Training: 19%]\tLoss: 0.084837\n",
            "Epoch: 2 [Training: 22%]\tLoss: 0.182452\n",
            "Epoch: 2 [Training: 26%]\tLoss: 0.049553\n",
            "Epoch: 2 [Training: 29%]\tLoss: 0.151087\n",
            "Epoch: 2 [Training: 32%]\tLoss: 0.084906\n",
            "Epoch: 2 [Training: 35%]\tLoss: 0.094218\n",
            "Epoch: 2 [Training: 38%]\tLoss: 0.065592\n",
            "Epoch: 2 [Training: 42%]\tLoss: 0.060736\n",
            "Epoch: 2 [Training: 45%]\tLoss: 0.113395\n",
            "Epoch: 2 [Training: 48%]\tLoss: 0.271212\n",
            "Epoch: 2 [Training: 51%]\tLoss: 0.101921\n",
            "Epoch: 2 [Training: 54%]\tLoss: 0.173669\n",
            "Epoch: 2 [Training: 58%]\tLoss: 0.088893\n",
            "Epoch: 2 [Training: 61%]\tLoss: 0.111194\n",
            "Epoch: 2 [Training: 64%]\tLoss: 0.068401\n",
            "Epoch: 2 [Training: 67%]\tLoss: 0.107188\n",
            "Epoch: 2 [Training: 70%]\tLoss: 0.083004\n",
            "Epoch: 2 [Training: 74%]\tLoss: 0.080349\n",
            "Epoch: 2 [Training: 77%]\tLoss: 0.244389\n",
            "Epoch: 2 [Training: 80%]\tLoss: 0.046849\n",
            "Epoch: 2 [Training: 83%]\tLoss: 0.149385\n",
            "Epoch: 2 [Training: 86%]\tLoss: 0.139488\n",
            "Epoch: 2 [Training: 90%]\tLoss: 0.184230\n",
            "Epoch: 2 [Training: 93%]\tLoss: 0.106080\n",
            "Epoch: 2 [Training: 96%]\tLoss: 0.046988\n",
            "Epoch: 2 [Training: 99%]\tLoss: 0.160111\n",
            "\n",
            "Test set: Average loss: 0.0887, Accuracy: 9729/10000 (97%)\n",
            "\n",
            "Epoch: 3 [Training: 0%]\tLoss: 0.075827\n",
            "Epoch: 3 [Training: 3%]\tLoss: 0.161296\n",
            "Epoch: 3 [Training: 6%]\tLoss: 0.237699\n",
            "Epoch: 3 [Training: 10%]\tLoss: 0.037253\n",
            "Epoch: 3 [Training: 13%]\tLoss: 0.104562\n",
            "Epoch: 3 [Training: 16%]\tLoss: 0.129226\n",
            "Epoch: 3 [Training: 19%]\tLoss: 0.123610\n",
            "Epoch: 3 [Training: 22%]\tLoss: 0.103469\n",
            "Epoch: 3 [Training: 26%]\tLoss: 0.159737\n",
            "Epoch: 3 [Training: 29%]\tLoss: 0.008427\n",
            "Epoch: 3 [Training: 32%]\tLoss: 0.113668\n",
            "Epoch: 3 [Training: 35%]\tLoss: 0.149556\n",
            "Epoch: 3 [Training: 38%]\tLoss: 0.039675\n",
            "Epoch: 3 [Training: 42%]\tLoss: 0.084251\n",
            "Epoch: 3 [Training: 45%]\tLoss: 0.096474\n",
            "Epoch: 3 [Training: 48%]\tLoss: 0.122777\n",
            "Epoch: 3 [Training: 51%]\tLoss: 0.114699\n",
            "Epoch: 3 [Training: 54%]\tLoss: 0.231862\n",
            "Epoch: 3 [Training: 58%]\tLoss: 0.025004\n",
            "Epoch: 3 [Training: 61%]\tLoss: 0.051794\n",
            "Epoch: 3 [Training: 64%]\tLoss: 0.077895\n",
            "Epoch: 3 [Training: 67%]\tLoss: 0.100712\n",
            "Epoch: 3 [Training: 70%]\tLoss: 0.040279\n",
            "Epoch: 3 [Training: 74%]\tLoss: 0.039629\n",
            "Epoch: 3 [Training: 77%]\tLoss: 0.100749\n",
            "Epoch: 3 [Training: 80%]\tLoss: 0.047791\n",
            "Epoch: 3 [Training: 83%]\tLoss: 0.044902\n",
            "Epoch: 3 [Training: 86%]\tLoss: 0.099444\n",
            "Epoch: 3 [Training: 90%]\tLoss: 0.031936\n",
            "Epoch: 3 [Training: 93%]\tLoss: 0.116199\n",
            "Epoch: 3 [Training: 96%]\tLoss: 0.060028\n",
            "Epoch: 3 [Training: 99%]\tLoss: 0.039206\n",
            "\n",
            "Test set: Average loss: 0.0703, Accuracy: 9767/10000 (98%)\n",
            "\n",
            "Epoch: 4 [Training: 0%]\tLoss: 0.065491\n",
            "Epoch: 4 [Training: 3%]\tLoss: 0.036795\n",
            "Epoch: 4 [Training: 6%]\tLoss: 0.037869\n",
            "Epoch: 4 [Training: 10%]\tLoss: 0.034986\n",
            "Epoch: 4 [Training: 13%]\tLoss: 0.017871\n",
            "Epoch: 4 [Training: 16%]\tLoss: 0.041578\n",
            "Epoch: 4 [Training: 19%]\tLoss: 0.013988\n",
            "Epoch: 4 [Training: 22%]\tLoss: 0.081009\n",
            "Epoch: 4 [Training: 26%]\tLoss: 0.033696\n",
            "Epoch: 4 [Training: 29%]\tLoss: 0.015685\n",
            "Epoch: 4 [Training: 32%]\tLoss: 0.035993\n",
            "Epoch: 4 [Training: 35%]\tLoss: 0.021864\n",
            "Epoch: 4 [Training: 38%]\tLoss: 0.066268\n",
            "Epoch: 4 [Training: 42%]\tLoss: 0.031693\n",
            "Epoch: 4 [Training: 45%]\tLoss: 0.037409\n",
            "Epoch: 4 [Training: 48%]\tLoss: 0.050130\n",
            "Epoch: 4 [Training: 51%]\tLoss: 0.061024\n",
            "Epoch: 4 [Training: 54%]\tLoss: 0.043057\n",
            "Epoch: 4 [Training: 58%]\tLoss: 0.053999\n",
            "Epoch: 4 [Training: 61%]\tLoss: 0.162298\n",
            "Epoch: 4 [Training: 64%]\tLoss: 0.015968\n",
            "Epoch: 4 [Training: 67%]\tLoss: 0.005860\n",
            "Epoch: 4 [Training: 70%]\tLoss: 0.161309\n",
            "Epoch: 4 [Training: 74%]\tLoss: 0.029399\n",
            "Epoch: 4 [Training: 77%]\tLoss: 0.071565\n",
            "Epoch: 4 [Training: 80%]\tLoss: 0.025716\n",
            "Epoch: 4 [Training: 83%]\tLoss: 0.023607\n",
            "Epoch: 4 [Training: 86%]\tLoss: 0.011631\n",
            "Epoch: 4 [Training: 90%]\tLoss: 0.035206\n",
            "Epoch: 4 [Training: 93%]\tLoss: 0.181208\n",
            "Epoch: 4 [Training: 96%]\tLoss: 0.017824\n",
            "Epoch: 4 [Training: 99%]\tLoss: 0.029918\n",
            "\n",
            "Test set: Average loss: 0.0580, Accuracy: 9819/10000 (98%)\n",
            "\n",
            "Epoch: 5 [Training: 0%]\tLoss: 0.102732\n",
            "Epoch: 5 [Training: 3%]\tLoss: 0.096311\n",
            "Epoch: 5 [Training: 6%]\tLoss: 0.038306\n",
            "Epoch: 5 [Training: 10%]\tLoss: 0.056382\n",
            "Epoch: 5 [Training: 13%]\tLoss: 0.085524\n",
            "Epoch: 5 [Training: 16%]\tLoss: 0.012803\n",
            "Epoch: 5 [Training: 19%]\tLoss: 0.016622\n",
            "Epoch: 5 [Training: 22%]\tLoss: 0.019214\n",
            "Epoch: 5 [Training: 26%]\tLoss: 0.007603\n",
            "Epoch: 5 [Training: 29%]\tLoss: 0.035317\n",
            "Epoch: 5 [Training: 32%]\tLoss: 0.077559\n",
            "Epoch: 5 [Training: 35%]\tLoss: 0.036228\n",
            "Epoch: 5 [Training: 38%]\tLoss: 0.019867\n",
            "Epoch: 5 [Training: 42%]\tLoss: 0.024125\n",
            "Epoch: 5 [Training: 45%]\tLoss: 0.040928\n",
            "Epoch: 5 [Training: 48%]\tLoss: 0.193936\n",
            "Epoch: 5 [Training: 51%]\tLoss: 0.121376\n",
            "Epoch: 5 [Training: 54%]\tLoss: 0.031555\n",
            "Epoch: 5 [Training: 58%]\tLoss: 0.095988\n",
            "Epoch: 5 [Training: 61%]\tLoss: 0.063557\n",
            "Epoch: 5 [Training: 64%]\tLoss: 0.043185\n",
            "Epoch: 5 [Training: 67%]\tLoss: 0.069780\n",
            "Epoch: 5 [Training: 70%]\tLoss: 0.037366\n",
            "Epoch: 5 [Training: 74%]\tLoss: 0.106188\n",
            "Epoch: 5 [Training: 77%]\tLoss: 0.015369\n",
            "Epoch: 5 [Training: 80%]\tLoss: 0.034462\n",
            "Epoch: 5 [Training: 83%]\tLoss: 0.013781\n",
            "Epoch: 5 [Training: 86%]\tLoss: 0.018858\n",
            "Epoch: 5 [Training: 90%]\tLoss: 0.009421\n",
            "Epoch: 5 [Training: 93%]\tLoss: 0.023459\n",
            "Epoch: 5 [Training: 96%]\tLoss: 0.084255\n",
            "Epoch: 5 [Training: 99%]\tLoss: 0.015044\n",
            "\n",
            "Test set: Average loss: 0.0483, Accuracy: 9851/10000 (99%)\n",
            "\n",
            "Epoch: 6 [Training: 0%]\tLoss: 0.028292\n",
            "Epoch: 6 [Training: 3%]\tLoss: 0.040786\n",
            "Epoch: 6 [Training: 6%]\tLoss: 0.158826\n",
            "Epoch: 6 [Training: 10%]\tLoss: 0.071430\n",
            "Epoch: 6 [Training: 13%]\tLoss: 0.005204\n",
            "Epoch: 6 [Training: 16%]\tLoss: 0.028649\n",
            "Epoch: 6 [Training: 19%]\tLoss: 0.027182\n",
            "Epoch: 6 [Training: 22%]\tLoss: 0.055595\n",
            "Epoch: 6 [Training: 26%]\tLoss: 0.034825\n",
            "Epoch: 6 [Training: 29%]\tLoss: 0.019257\n",
            "Epoch: 6 [Training: 32%]\tLoss: 0.115385\n",
            "Epoch: 6 [Training: 35%]\tLoss: 0.109056\n",
            "Epoch: 6 [Training: 38%]\tLoss: 0.093831\n",
            "Epoch: 6 [Training: 42%]\tLoss: 0.007013\n",
            "Epoch: 6 [Training: 45%]\tLoss: 0.042249\n",
            "Epoch: 6 [Training: 48%]\tLoss: 0.010779\n",
            "Epoch: 6 [Training: 51%]\tLoss: 0.014447\n",
            "Epoch: 6 [Training: 54%]\tLoss: 0.016282\n",
            "Epoch: 6 [Training: 58%]\tLoss: 0.013096\n",
            "Epoch: 6 [Training: 61%]\tLoss: 0.023653\n",
            "Epoch: 6 [Training: 64%]\tLoss: 0.100100\n",
            "Epoch: 6 [Training: 67%]\tLoss: 0.078699\n",
            "Epoch: 6 [Training: 70%]\tLoss: 0.009189\n",
            "Epoch: 6 [Training: 74%]\tLoss: 0.007848\n",
            "Epoch: 6 [Training: 77%]\tLoss: 0.053629\n",
            "Epoch: 6 [Training: 80%]\tLoss: 0.038823\n",
            "Epoch: 6 [Training: 83%]\tLoss: 0.022322\n",
            "Epoch: 6 [Training: 86%]\tLoss: 0.048147\n",
            "Epoch: 6 [Training: 90%]\tLoss: 0.022462\n",
            "Epoch: 6 [Training: 93%]\tLoss: 0.090198\n",
            "Epoch: 6 [Training: 96%]\tLoss: 0.015506\n",
            "Epoch: 6 [Training: 99%]\tLoss: 0.031457\n",
            "\n",
            "Test set: Average loss: 0.0565, Accuracy: 9830/10000 (98%)\n",
            "\n",
            "Epoch: 7 [Training: 0%]\tLoss: 0.017916\n",
            "Epoch: 7 [Training: 3%]\tLoss: 0.018246\n",
            "Epoch: 7 [Training: 6%]\tLoss: 0.014554\n",
            "Epoch: 7 [Training: 10%]\tLoss: 0.015955\n",
            "Epoch: 7 [Training: 13%]\tLoss: 0.060531\n",
            "Epoch: 7 [Training: 16%]\tLoss: 0.030211\n",
            "Epoch: 7 [Training: 19%]\tLoss: 0.048051\n",
            "Epoch: 7 [Training: 22%]\tLoss: 0.021297\n",
            "Epoch: 7 [Training: 26%]\tLoss: 0.049018\n",
            "Epoch: 7 [Training: 29%]\tLoss: 0.068309\n",
            "Epoch: 7 [Training: 32%]\tLoss: 0.037900\n",
            "Epoch: 7 [Training: 35%]\tLoss: 0.008780\n",
            "Epoch: 7 [Training: 38%]\tLoss: 0.014413\n",
            "Epoch: 7 [Training: 42%]\tLoss: 0.126151\n",
            "Epoch: 7 [Training: 45%]\tLoss: 0.010207\n",
            "Epoch: 7 [Training: 48%]\tLoss: 0.017150\n",
            "Epoch: 7 [Training: 51%]\tLoss: 0.070563\n",
            "Epoch: 7 [Training: 54%]\tLoss: 0.022589\n",
            "Epoch: 7 [Training: 58%]\tLoss: 0.087373\n",
            "Epoch: 7 [Training: 61%]\tLoss: 0.042722\n",
            "Epoch: 7 [Training: 64%]\tLoss: 0.035513\n",
            "Epoch: 7 [Training: 67%]\tLoss: 0.075887\n",
            "Epoch: 7 [Training: 70%]\tLoss: 0.028999\n",
            "Epoch: 7 [Training: 74%]\tLoss: 0.020384\n",
            "Epoch: 7 [Training: 77%]\tLoss: 0.013011\n",
            "Epoch: 7 [Training: 80%]\tLoss: 0.022649\n",
            "Epoch: 7 [Training: 83%]\tLoss: 0.028599\n",
            "Epoch: 7 [Training: 86%]\tLoss: 0.016029\n",
            "Epoch: 7 [Training: 90%]\tLoss: 0.014252\n",
            "Epoch: 7 [Training: 93%]\tLoss: 0.021707\n",
            "Epoch: 7 [Training: 96%]\tLoss: 0.080078\n",
            "Epoch: 7 [Training: 99%]\tLoss: 0.022635\n",
            "\n",
            "Test set: Average loss: 0.0428, Accuracy: 9868/10000 (99%)\n",
            "\n",
            "Epoch: 8 [Training: 0%]\tLoss: 0.024620\n",
            "Epoch: 8 [Training: 3%]\tLoss: 0.021604\n",
            "Epoch: 8 [Training: 6%]\tLoss: 0.105299\n",
            "Epoch: 8 [Training: 10%]\tLoss: 0.077189\n",
            "Epoch: 8 [Training: 13%]\tLoss: 0.015752\n",
            "Epoch: 8 [Training: 16%]\tLoss: 0.019431\n",
            "Epoch: 8 [Training: 19%]\tLoss: 0.025672\n",
            "Epoch: 8 [Training: 22%]\tLoss: 0.072418\n",
            "Epoch: 8 [Training: 26%]\tLoss: 0.053525\n",
            "Epoch: 8 [Training: 29%]\tLoss: 0.018276\n",
            "Epoch: 8 [Training: 32%]\tLoss: 0.018204\n",
            "Epoch: 8 [Training: 35%]\tLoss: 0.007216\n",
            "Epoch: 8 [Training: 38%]\tLoss: 0.014287\n",
            "Epoch: 8 [Training: 42%]\tLoss: 0.018759\n",
            "Epoch: 8 [Training: 45%]\tLoss: 0.040494\n",
            "Epoch: 8 [Training: 48%]\tLoss: 0.079137\n",
            "Epoch: 8 [Training: 51%]\tLoss: 0.102865\n",
            "Epoch: 8 [Training: 54%]\tLoss: 0.009307\n",
            "Epoch: 8 [Training: 58%]\tLoss: 0.036281\n",
            "Epoch: 8 [Training: 61%]\tLoss: 0.009463\n",
            "Epoch: 8 [Training: 64%]\tLoss: 0.076908\n",
            "Epoch: 8 [Training: 67%]\tLoss: 0.010424\n",
            "Epoch: 8 [Training: 70%]\tLoss: 0.081126\n",
            "Epoch: 8 [Training: 74%]\tLoss: 0.068184\n",
            "Epoch: 8 [Training: 77%]\tLoss: 0.110113\n",
            "Epoch: 8 [Training: 80%]\tLoss: 0.056940\n",
            "Epoch: 8 [Training: 83%]\tLoss: 0.052988\n",
            "Epoch: 8 [Training: 86%]\tLoss: 0.031504\n",
            "Epoch: 8 [Training: 90%]\tLoss: 0.032932\n",
            "Epoch: 8 [Training: 93%]\tLoss: 0.033360\n",
            "Epoch: 8 [Training: 96%]\tLoss: 0.073392\n",
            "Epoch: 8 [Training: 99%]\tLoss: 0.009500\n",
            "\n",
            "Test set: Average loss: 0.0400, Accuracy: 9877/10000 (99%)\n",
            "\n",
            "Epoch: 9 [Training: 0%]\tLoss: 0.025556\n",
            "Epoch: 9 [Training: 3%]\tLoss: 0.012549\n",
            "Epoch: 9 [Training: 6%]\tLoss: 0.009996\n",
            "Epoch: 9 [Training: 10%]\tLoss: 0.071745\n",
            "Epoch: 9 [Training: 13%]\tLoss: 0.018487\n",
            "Epoch: 9 [Training: 16%]\tLoss: 0.018431\n",
            "Epoch: 9 [Training: 19%]\tLoss: 0.024196\n",
            "Epoch: 9 [Training: 22%]\tLoss: 0.076757\n",
            "Epoch: 9 [Training: 26%]\tLoss: 0.027459\n",
            "Epoch: 9 [Training: 29%]\tLoss: 0.112122\n",
            "Epoch: 9 [Training: 32%]\tLoss: 0.080051\n",
            "Epoch: 9 [Training: 35%]\tLoss: 0.007304\n",
            "Epoch: 9 [Training: 38%]\tLoss: 0.011727\n",
            "Epoch: 9 [Training: 42%]\tLoss: 0.007299\n",
            "Epoch: 9 [Training: 45%]\tLoss: 0.003376\n",
            "Epoch: 9 [Training: 48%]\tLoss: 0.019225\n",
            "Epoch: 9 [Training: 51%]\tLoss: 0.014752\n",
            "Epoch: 9 [Training: 54%]\tLoss: 0.024964\n",
            "Epoch: 9 [Training: 58%]\tLoss: 0.008432\n",
            "Epoch: 9 [Training: 61%]\tLoss: 0.012906\n",
            "Epoch: 9 [Training: 64%]\tLoss: 0.011894\n",
            "Epoch: 9 [Training: 67%]\tLoss: 0.021994\n",
            "Epoch: 9 [Training: 70%]\tLoss: 0.007961\n",
            "Epoch: 9 [Training: 74%]\tLoss: 0.058527\n",
            "Epoch: 9 [Training: 77%]\tLoss: 0.044201\n",
            "Epoch: 9 [Training: 80%]\tLoss: 0.028370\n",
            "Epoch: 9 [Training: 83%]\tLoss: 0.007401\n",
            "Epoch: 9 [Training: 86%]\tLoss: 0.012486\n",
            "Epoch: 9 [Training: 90%]\tLoss: 0.011763\n",
            "Epoch: 9 [Training: 93%]\tLoss: 0.052039\n",
            "Epoch: 9 [Training: 96%]\tLoss: 0.019130\n",
            "Epoch: 9 [Training: 99%]\tLoss: 0.026157\n",
            "\n",
            "Test set: Average loss: 0.0386, Accuracy: 9880/10000 (99%)\n",
            "\n",
            "Epoch: 10 [Training: 0%]\tLoss: 0.011048\n",
            "Epoch: 10 [Training: 3%]\tLoss: 0.010391\n",
            "Epoch: 10 [Training: 6%]\tLoss: 0.060123\n",
            "Epoch: 10 [Training: 10%]\tLoss: 0.049831\n",
            "Epoch: 10 [Training: 13%]\tLoss: 0.036571\n",
            "Epoch: 10 [Training: 16%]\tLoss: 0.099952\n",
            "Epoch: 10 [Training: 19%]\tLoss: 0.029475\n",
            "Epoch: 10 [Training: 22%]\tLoss: 0.007962\n",
            "Epoch: 10 [Training: 26%]\tLoss: 0.092033\n",
            "Epoch: 10 [Training: 29%]\tLoss: 0.016786\n",
            "Epoch: 10 [Training: 32%]\tLoss: 0.014392\n",
            "Epoch: 10 [Training: 35%]\tLoss: 0.049714\n",
            "Epoch: 10 [Training: 38%]\tLoss: 0.019074\n",
            "Epoch: 10 [Training: 42%]\tLoss: 0.089127\n",
            "Epoch: 10 [Training: 45%]\tLoss: 0.067557\n",
            "Epoch: 10 [Training: 48%]\tLoss: 0.054713\n",
            "Epoch: 10 [Training: 51%]\tLoss: 0.004923\n",
            "Epoch: 10 [Training: 54%]\tLoss: 0.006377\n",
            "Epoch: 10 [Training: 58%]\tLoss: 0.044104\n",
            "Epoch: 10 [Training: 61%]\tLoss: 0.016562\n",
            "Epoch: 10 [Training: 64%]\tLoss: 0.054276\n",
            "Epoch: 10 [Training: 67%]\tLoss: 0.003278\n",
            "Epoch: 10 [Training: 70%]\tLoss: 0.010208\n",
            "Epoch: 10 [Training: 74%]\tLoss: 0.026954\n",
            "Epoch: 10 [Training: 77%]\tLoss: 0.016133\n",
            "Epoch: 10 [Training: 80%]\tLoss: 0.037958\n",
            "Epoch: 10 [Training: 83%]\tLoss: 0.056679\n",
            "Epoch: 10 [Training: 86%]\tLoss: 0.028698\n",
            "Epoch: 10 [Training: 90%]\tLoss: 0.001801\n",
            "Epoch: 10 [Training: 93%]\tLoss: 0.029130\n",
            "Epoch: 10 [Training: 96%]\tLoss: 0.004416\n",
            "Epoch: 10 [Training: 99%]\tLoss: 0.118573\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 9890/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14Q8wcO55gF6",
        "colab_type": "code",
        "outputId": "3288d7f7-bbde-444f-8d23-c0f9f82712ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Accuracy Obtained {:.4f}%\".format( 100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Obtained 98.9000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKZmovMQuh_3",
        "colab_type": "text"
      },
      "source": [
        "<h3> Do Check out [The Future of AI: Federated Learning](https://medium.com/secure-and-private-ai-writing-challenge/the-future-of-ai-federated-learning-fe977cdd3c62)"
      ]
    }
  ]
}